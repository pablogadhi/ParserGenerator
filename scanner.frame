/*----------------------------------------------------------------------
scanner.h Specification
-----------------------------------------------------------------------*/

-->begin
#ifndef SCANNER_H
#define SCANNER_H

#include "error.h"
#include "state_machine.h"
#include <fstream>
#include <string>
#include <unordered_map>
#include <vector>

using namespace std;

template <class T> class Token
{
  private:
    string t_name;
    T t_val;

  public:
    Token();
    Token(string, T);
    ~Token();
    void set_name(string);
    string name();
    T value();
    bool operator==(Token &t)
    {
        return this->t_val == t.value();
    }
    bool operator<(Token<T> &t)
    {
        return t_name < t.name();
    }
};

class SymbolTable
{
  private:
    unordered_map<string, Set<char>> char_set_map;
    unordered_map<string, string> keywords_map;

  public:
    void add_char_set(string, Set<char>);
    void add_keyword(string, string);
    unordered_map<string, Set<char>> &char_sets();
    unordered_map<string, string> &keywords();
};

class Scanner
{
  private:
    ifstream file_buffer;
    string buffer_0;
    string buffer_1;
    string current_buffer;
    string prev_buffer;
    string::iterator forward;
    int lexeme_begin_idx;
    Token<string> c_token;
    Token<string> n_token;
    DFA finder = DFA();
    SymbolTable s_table;
    int line = 1;
    int column = 1;
    vector<Error> error_list;

    void read_into_string_buffer(string &);

  public:
    Scanner();
    Scanner(string);
    ~Scanner();

    void set_finder(DFA);
    DFA &get_finder();
    SymbolTable &symbols();
    Token<string> scan();
    void next_char();
    char peek_char();
    void ignore_all_blank_chars();
    Token<string> next_token();
    Token<string> current();
    Token<string> look_ahead();
    vector<Error> errors();
    void lex_error();
};

#endif
-->implementation

/*----------------------------------------------------------------------
scanner.cpp Specification
-----------------------------------------------------------------------*/

-->begin
#include "scanner.h"
#include "utils.h"
#include <iostream>

template <class T> Token<T>::Token()
{
}

template <class T> Token<T>::Token(string name, T val) : t_name(name), t_val(val)
{
}

template <class T> Token<T>::~Token()
{
}

template <class T> string Token<T>::name()
{
    return t_name;
}

template <class T> T Token<T>::value()
{
    return t_val;
}

void SymbolTable::add_char_set(string set_name, Set<char> new_set)
{
    char_set_map[set_name] = new_set;
}

void SymbolTable::add_keyword(string name, string new_keyword)
{
    keywords_map[name] = new_keyword;
}

unordered_map<string, Set<char>> &SymbolTable::char_sets()
{
    return char_set_map;
}

unordered_map<string, string> &SymbolTable::keywords()
{
    return keywords_map;
}

SymbolTable &Scanner::symbols()
{
    return s_table;
}

Scanner::Scanner()
{
}

Scanner::Scanner(string file_name)
{
    file_buffer.open(file_name);
    if (!file_buffer.is_open())
    {
        cout << "Failed to open the file!" << endl;
        throw std::logic_error("Failed to open the file!");
    }

    // Initialize Character Map
-->char_sets_decl

    // Initialize keywords
-->keywords_decl

    // Initialize DFA
    unordered_map<int, shared_ptr<State>> state_ptrs;
-->dfa_decl

    finder = DFA(state_ptrs[0], nullptr);

    read_into_string_buffer(buffer_0);
    current_buffer = buffer_0;
    prev_buffer = buffer_0;
    lexeme_begin_idx = 0;
    forward = current_buffer.begin();
}

Scanner::~Scanner()
{
    file_buffer.close();
}

void Scanner::set_finder(DFA dfa)
{
    finder = dfa;
}

DFA &Scanner::get_finder()
{
    return finder;
}

void Scanner::read_into_string_buffer(string &out_buffer)
{
    char *temp_buffer = new char[2048];
    file_buffer.read(temp_buffer, 2048);
    out_buffer = string(temp_buffer);
    // out_buffer = out_buffer + '\0';
    delete temp_buffer;
}

void Scanner::next_char()
{
    if (*forward == '\n')
    {
        line++;
        column = 1;
    }

    forward++;
    column++;
    if (forward == current_buffer.end())
    {
        if (file_buffer.eof())
        {
            forward = current_buffer.end();
        }
        else if (current_buffer == buffer_0)
        {
            read_into_string_buffer(buffer_1);
            current_buffer = buffer_1;
            forward = current_buffer.begin();
        }
        else if (current_buffer == buffer_1)
        {
            read_into_string_buffer(buffer_0);
            current_buffer = buffer_0;
            forward = current_buffer.begin();
        }
    }
}

char Scanner::peek_char()
{
    char peeked;
    forward++;
    if (forward == current_buffer.end())
    {
        peeked = file_buffer.peek();
    }
    else
    {
        peeked = *forward;
    }
    forward--;
    return peeked;
}

Token<string> Scanner::scan()
{
    c_token = n_token;
    n_token = next_token();
    while (n_token.name() == "<ERROR>")
    {
        n_token = next_token();
    }
    return c_token;
}

void Scanner::ignore_all_blank_chars()
{
    if (s_table.char_sets().find("<IGNORE>") != s_table.char_sets().end())
    {
        while (s_table.char_sets()["<IGNORE>"].has_item(*forward) != -1)
        {
            next_char();
        }
    }
}

Token<string> Scanner::next_token()
{
    prev_buffer = current_buffer;
    ignore_all_blank_chars();

    // Handle EOF
    if (forward == current_buffer.end())
    {
        return Token<string>("<EOF>", string(1, '\0'));
    }

    lexeme_begin_idx = forward - current_buffer.begin();

    // Move until an accepting state is found
    bool accepting_found = false;
    State peeked_state = State(-2);
    while (!accepting_found || peeked_state.name() != -1)
    {
        finder.move(*forward);
        if (finder.current().is_accepting())
        {
            accepting_found = true;
            if (finder.is_path_non_recursive())
            {
                peeked_state = State();
            }
            else
            {
                peeked_state = finder.peek_move(peek_char());
            }
        }
        else if (finder.current().name() == -1)
        {
            lex_error();
            finder.reset_movements();
            next_char();
            return Token<string>("<ERROR>", "");
        }
        next_char();
    }

    string lexeme_str;
    if (current_buffer != prev_buffer)
    {
        auto l_end_len = forward - current_buffer.begin();
        lexeme_str = prev_buffer.substr(lexeme_begin_idx) + current_buffer.substr(0, l_end_len);
    }
    else
    {
        auto l_len = forward - current_buffer.begin() - lexeme_begin_idx;
        lexeme_str = current_buffer.substr(lexeme_begin_idx, l_len);
    }

    string token_name = finder.current().reference_name();
    if (s_table.keywords().find(lexeme_str) != s_table.keywords().end())
    {
        ignore_all_blank_chars();
        // QuickFix for reading keywords in the COCOL grammar
        if (*forward != '=')
        {
            token_name = s_table.keywords()[lexeme_str];
        }
    }

    finder.reset_movements();
    return Token<string>(token_name, lexeme_str);
}

Token<string> Scanner::current()
{
    return c_token;
}

Token<string> Scanner::look_ahead()
{
    return n_token;
}

vector<Error> Scanner::errors()
{
    return error_list;
}

void Scanner::lex_error()
{
    auto error = Error(line, column, "Unexpected char: " + char_to_str(*forward, true));
    cout << "\nLEXICAL ERROR found in line " << error.line() << " and column " << error.column() << ": "
         << error.value() << endl
         << endl;
    error_list.push_back(error);
}

template class Token<string>;
template class Token<Set<char>>;
-->end